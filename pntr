# -*- coding: utf-8 -*-
"""
Created on Sun Jul 28 18:22:46 2019

@author: testworld
"""

import tensorflow as tf
import datapreproc as dataset
import pandas as pd
import os

from matplotlib import pyplot as plt
from matplotlib.pylab import date2num
"""
隐藏层个数 前后必须一致

如果attention的输出直接作为输入，
那么这个维度也必须保持一致

attention的维度 也即隐藏层个数
而 目标输出的维度是 输入的维度，所以这三个参数必须保持一致

这里用一个线性变换，将目标解开


"""
modelname = 'ModelOne'
inputsize = 8# output size
EncoderHiddenNum = 16# 
DecoderHiddenNum = 7#  全解开
batch_size = 33
EnSeqLth = 20
DeSeqLth = 12
times = DeSeqLth# 这个是必须相等，保证其具有相同的结果
TrainStep = 200
ModelSaveFp = 'C:/Files/huobiv2.0/Models/'







modelname = 'ModelOne'
inputsize = 8# output size
EncoderHiddenNum = 16# 
DecoderHiddenNum = 7#  全解开
batch_size = 5
EnSeqLth = 60
DeSeqLth = 2
times = DeSeqLth# 这个是必须相等，保证其具有相同的结果
TrainStep = 100000

ModelSaveFp = 'C:/Files/huobiv2.0/Models/Model2/'

if not os.path.exists(ModelSaveFp):
    os.makedirs(ModelSaveFp)
try:
    a = a+2
except :
    a = 1


with tf.variable_scope('try'+str(a)) as gvs:
    ####################################################
    encode_inputs   = [tf.placeholder(tf.float32, [batch_size, inputsize], name="EncoderInput%d" % i) for i in range(EnSeqLth)]
    decoder_inputs  = tf.placeholder(tf.float32, [batch_size, inputsize], name="DecoderInput")
    decoder_targets = [tf.placeholder(tf.float32, [batch_size, inputsize], name="DecoderTarget%d" % i) for i in range(DeSeqLth)]
    ####################################################
    EnCell = tf.nn.rnn_cell.GRUCell(EncoderHiddenNum)
    DeCell = tf.nn.rnn_cell.GRUCell(DecoderHiddenNum)
    ####################################################
    encoder_outputs, final_state = tf.nn.static_rnn(EnCell,encode_inputs, dtype=tf.float32)
    ####################################################
    top_states = [tf.reshape(e, [-1, 1, EnCell.output_size]) for e in encoder_outputs]
    attention_states = tf.concat(axis=1, values=top_states)
    outputs = [decoder_inputs]
    state = [final_state]
    TranOutput2Att = tf.get_variable('Output2Attention',(EncoderHiddenNum,DecoderHiddenNum))
    state = [tf.matmul(final_state,TranOutput2Att)]
    with tf.variable_scope('OutPut',reuse = tf.AUTO_REUSE) as sp:
        for _ in range(times):
            ####################################################
            t_outputs, t_state = tf.contrib.legacy_seq2seq.attention_decoder([outputs[0]],state[-1],attention_states,DeCell,num_heads = 1)
            TranHid2Output = tf.get_variable('Hidden2Output',(DecoderHiddenNum,inputsize))
            outputs.append(tf.matmul(t_outputs[-1],TranHid2Output))
            state.append(t_state)
            ####################################################
    ####################################################
    loss = 0.0
    for output, target in zip(outputs[1:], decoder_targets):
        #loss += tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=target)
        for i in [1,3,4,5,7]:
            loss += tf.nn.softmax_cross_entropy_with_logits(logits=output[:,i], labels=target[:,i])
    sumloss = tf.reduce_mean(loss)
    #####################:###############################
    data = dataset.DataProc()
    Data = data.GenSampleType1(Symbol = 'btmusdt',Period = '1min')
    
    def create_feed_dict(InputX, InputTarget, TargetY):
        
        feed_dict = {}
        for placeholder, data in zip(encode_inputs, InputX): 
            feed_dict[placeholder] = data
    
        #for placeholder, data in zip(decoder_inputs, InputTarget):
        feed_dict[decoder_inputs] = InputTarget
    
        for placeholder, data in zip(decoder_targets, TargetY):
            feed_dict[placeholder] = data
    
        return feed_dict
    ####################################################
    with tf.Session() as sess:
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        #merged = tf.summary.merge_all()
        #writer = tf.summary.FileWriter("/tmp/pointer_logs", sess.graph)
        optimizer = tf.train.AdamOptimizer()
        train_op = optimizer.minimize(sumloss)
        init = tf.global_variables_initializer()
        sess.run(init)
        saver = tf.train.Saver()
        try:
            saver.restore(sess, ModelSaveFp + modelname + ".ckpt")
            print("加载图成功")
        except :
            print("使用新的图")            
            pass
        def recoverData(data):
            reData = []
            dataNum = data[0].shape[0]
            seqlth  = len(data)
            for i in range(dataNum):
                reData.append([data[j][i] for  j in range(seqlth)])
            return reData
        
        
        def myplot(data1,data2,fea,time):
            pdp = pd.DataFrame(data1,columns = fea)
            pdr = pd.DataFrame(data2,columns = fea)
            plt.plot(time,pdp['close'], color='cyan', label='pred')
            plt.plot(time,pdr['close'], 'b', label='real')#'b'指：color='blue'
            plt.legend('pred','real')
            plt.show()
            pass
        
        def Compare(pred,real,time,fea):
            pdata = recoverData(pred)
            rdata = recoverData(real)
            for p,r,t in zip(pdata,rdata,time):
                myplot(p,r,fea,t)
                break
            pass
        totalloss = []
        for i in range(TrainStep):
            if i % 10 == 0:
                print("step : %d" % i)
            InputX,InputTarget,TargetY,_ = data.NextBatch(Data,batchsize = batch_size,EncodeStep = EnSeqLth,DecodeStep = DeSeqLth)
            feed_dict = create_feed_dict(InputX, InputTarget, TargetY)
            sess.run(train_op,feed_dict = feed_dict)
            totalloss.append(sess.run(sumloss,feed_dict = feed_dict))
        #################################################### 绘制实际结果
            if i % 100 == 0:
                InputX,InputTarget,TargetY,TargetY_t = data.NextBatch(Data,batchsize = batch_size,EncodeStep = EnSeqLth,DecodeStep = DeSeqLth)
                sess.run(train_op,feed_dict = feed_dict)
                feed_dict = create_feed_dict(InputX, InputTarget, TargetY)
                pred = sess.run(outputs,feed_dict = feed_dict)
                fea = data.fea
                prd = Compare(pred[1:],TargetY,TargetY_t,fea)
                plt.plot(totalloss)
                plt.show()
        #################################################### 绘制实际结果
        save_path = saver.save(sess,  ModelSaveFp + modelname + ".ckpt")
























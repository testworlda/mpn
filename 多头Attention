摘要，在Attention机制中，特征抽取器除了典型的RNN和CNN架构外，还有一个非常重要的基于attention机制的transformer抽取器，这里简要实现该特征提取器
一下这种实现方式
综合了一下功能
残差，dropout，normalization
比较好的多头Attention机制的实现实现
##多头机制
def mulitAtt(inputs,num_units=None,num_heads = 16,dropout_rate = 0.09,is_train = True,seed = 111)
  with tf.variable_scope("attention")
    Q/K/V = tf.layer.dense(inputs,num_units,activation = tf.nn.relu)#[b,sl,nh]
    Q_/K_/V_ = tf.concat(tf.split(Q,num_heads,axis=2),axis=0)
    outputs = tf.matmul(Q_,tf.transpose(K_,[0,2,1]))
    outputs = outputs/(K_.get_shape().as_list()[-1]**0.5)
    outputs = tf.nn.softmax(outputs)  # num_heads*[]
    outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_train))#, seed=seed)
    outputs = tf.matmul(outputs, V_)  # num_heads*[batch_size, seq_length, n_hidden/num_heads]
    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # [batch_size, seq_length, n_hidden]
    outputs += inputs  # [batch_size, seq_length, n_hidden]
    outputs = tf.layers.batch_normalization(outputs, axis=2, training=is_training, name='ln',reuse=None)  # [batch_size, seq_length, n_hidden]
    return outputs
## 前馈网络
def feedforward(inputs, num_units=[2048, 512], is_training=True, seed=128):
    with tf.variable_scope("ffn"):#, reuse=None):
        # Inner layer
        params = {"inputs": inputs, "filters": num_units[0], "kernel_size": 1, "activation": tf.nn.relu,
                  "use_bias": True}
        outputs = tf.layers.conv1d(**params)
        # Readout layer
        params = {"inputs": outputs, "filters": num_units[1], "kernel_size": 1, "activation": None, "use_bias": True}
        outputs = tf.layers.conv1d(**params)
        # Residual connection
        outputs += inputs
        # Normalize
        outputs = tf.layers.batch_normalization(outputs, axis=2, training=is_training, name='ln',reuse=None)  # [batch_size, seq_length, n_hidden]
    return outputs    
 实现过程
W_embed = tf.get_variable("weights", [1, self.input_dimension, self.input_embed])
embedded_input = tf.nn.conv1d(inputs, W_embed, 1, "VALID", name="embedded_input")
enc = tf.layers.batch_normalization(self.embedded_input, axis=2, training=self.is_training,
                                                     name='layer_norm', reuse=None)
for i in range(self.num_stacks):  # num blocks
with tf.variable_scope("block_{}".format(i)):
enc = multihead_attention
enc = feedforward(self.enc, num_units=[4 * self.input_embed, self.input_embed])
encoder_output = enc 
return encoder_output
    
    
    
    
    
    
  
